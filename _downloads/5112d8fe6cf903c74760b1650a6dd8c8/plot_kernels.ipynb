{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Kernels\n\nThis tutorial aims to explain and visualize the kernels obtained from the data\nencoding methods implemented in the package as well as the quantum inspired\nkernels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n\nAs described in the `encodingstutorial` tutorial, the encoding methods\ncan be interpreted as feature maps of the form:\n\n\\begin{align}\\phi : \\mathcal{X} \\rightarrow \\mathcal{F}\\end{align}\n\nWhere the input $x$ is mapped from the input space into the feature\nspace.\n\nThe inner product of two inputs mapped to feature space defines a kernel via:\n\n\\begin{align}k(x,x'):= \\left<\\phi(x), \\phi(x')\\right>_\\mathcal{F}\\end{align}\n\nWhere $\\left<.,.\\right>_\\mathcal{F}$ is the inner product defined\non $\\mathcal{F}$. :cite:`hilbert2019`\n\nIn this case, as the inputs are being mapped into quantum states,\nthe kernel defined is of the form:\n\n\\begin{align}k(x,x')=\\left<\\phi(x)|\\phi(x')\\right>.\\end{align}\n\nBecause of this, the encoding methods can be used to define kernels with the\ninner product and use these with machine learning algorithms such as\nSupport-Vector Machines (SVMs). On top of that, the inner product can also be\nestimated with quantum subroutines with a time complexity of\n$O(\\log N)$, providing an exponential speed-up over\nthe classical calculation. For the mathematical reasoning behind the\nquantum inner product estimation, refer to the documentation for\n:meth:`skqlearn.utils.inner_product_estimation`.\n\nTwo methods have been defined for the encoding classes, `classic_kernel` and\n`quantum_kernel`, in order to use the kernels defined by the encoding methods\nwith implementations of SVMs such as\n`sklearn's <https://scikit-learn.org/stable/modules/svm.html#svm>`_.\nThese methods compute the Gram matrix of the input set of vectors.\n\nThroughout this tutorial, both the classical computation and quantum\nestimation of the kernels will be tested, so the quantum backend will\nbe initialized with one of the simulators provided by Qiskit.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skqlearn.utils import JobHandler\nfrom skqlearn.encoding import *\nfrom skqlearn.ml.kernels import SqueezingKernel\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom qiskit.providers.aer import AerSimulator\nfrom sklearn.svm import SVC\n\nJobHandler().configure(backend=AerSimulator(), shots=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basis Encoding\n\nBasis Encoding's feature map was:\n\n\\begin{align}\\phi : i\\rightarrow \\left|i\\right>\\end{align}\n\nTherefore, the kernel defined by the inner product is:\n\n\\begin{align}k(i, j) = \\left<\\phi(i)|\\phi(j)\\right> = \\left<i|j\\right> =\n    \\delta_{ij}\\end{align}\n\nWith $\\delta$ being the Kronecker delta, defined as\n$\\delta_{ij}=[i=j]$.\n\nWhen computing the Gram matrix of a set of vectors, the expected\nresult would be the identity matrix.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = np.array([[1], [2], [3], [4]])\nprint('Gram matrix with classical computation:')\nprint(BasisEncoding().classic_kernel(x, x))\nprint('Gram matrix with quantum estimation:')\nprint(BasisEncoding().quantum_kernel(x, x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because the next encodings can be applied to generic points in 2D space,\na couple of utility functions will be defined to help with the visualization\nof the results. Specifically, they will be used to visualize the decision\nboundaries of the SVM.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_meshgrid(x, y, h=.2, border=.1):\n    x_min, x_max = x.min() - border, x.max() + border\n    y_min, y_max = y.min() - border, y.max() + border\n    xx, yy = np.meshgrid(np.arange(x_min, x_max + h / 2, h),\n                         np.arange(y_min, y_max + h / 2, h))\n    return xx, yy\n\n\ndef plot_contours(ax, clf, xx, yy, **params):\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    out = ax.contourf(xx, yy, Z, **params)\n    return out\n\n\ndef plot_individual(title, clf, X0, X1):\n    xx, yy = make_meshgrid(X0, X1)\n    plt.title(title)\n    plot_contours(plt, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n    plt.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=60, edgecolors='k')\n    plt.xlabel(r'$X_1$')\n    plt.ylabel(r'$X_2$')\n    plt.show()\n\n\ndef plot_comparison(title, clf_c, clf_q, X0, X1):\n    xx, yy = make_meshgrid(X0, X1)\n    fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    fig.suptitle(title)\n    plot_contours(ax1, clf_c, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n    plot_contours(ax2, clf_q, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n    ax1.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=60, edgecolors='k')\n    ax2.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=60, edgecolors='k')\n    ax1.set_title('Classic Computation')\n    ax1.set(xlabel=r'$X_1$', ylabel=r'$X_2$')\n    ax1.set_aspect('equal', 'box')\n    ax2.set_title('Quantum Estimation')\n    ax2.set(xlabel=r'$X_1$', ylabel=r'$X_2$')\n    ax2.set_aspect('equal', 'box')\n    plt.subplots_adjust(left=0.13, bottom=0.01, right=0.95, top=0.99, wspace=0.12)\n    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to use a simple example which is not linearly separable, the\nproposed data for the SVM to classify is the XOR problem described with\nbipolar inputs. This is chosen over its binary representation because\nAmplitude Encoding is unable to encode empty vectors (where all components\nequal to 0).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])\ny = np.array([-1, 1, 1, -1])\nX1, X2 = x[:, 0], x[:, 1]\n\nplt.scatter(X1, X2, c=y, cmap=plt.cm.coolwarm, s=60, edgecolors='k')\nplt.xlabel(r'$X_1$')\nplt.ylabel(r'$X_2$')\nplt.title('XOR problem with bipolar values')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Amplitude Encoding\n\nAmplitude Encoding's feature map was:\n\n\\begin{align}\\phi:\\boldsymbol{x}\\rightarrow\\left|\\psi_\\boldsymbol{x}\\right>=\n   \\sum_{i=1}^{N}\\frac{1}{|\\boldsymbol{x}|}x_i\\left|i-1\\right>\\end{align}\n\nTherefore, the kernel defined by the inner product is:\n\n\\begin{align}k(\\boldsymbol{x}, \\boldsymbol{x'}) = \\left<\\psi_{\\boldsymbol{x}}|\n   \\psi_{\\boldsymbol{x'}}\\right> = \\frac{1}{|\\boldsymbol{x}|\n   |\\boldsymbol{x'}|}\\boldsymbol{x}^T\\boldsymbol{x'}\\end{align}\n\nBy applying a correction of $|\\boldsymbol{x}||\\boldsymbol{x'}|$ this\ncan be used to estimate the linear kernel.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf_amp_c = SVC(kernel=AmplitudeEncoding(degree=1).classic_kernel)\nclf_amp_c.fit(x, y)\nclf_amp_q = SVC(kernel=AmplitudeEncoding(degree=1).quantum_kernel)\nclf_amp_q.fit(x, y)\n\nplot_comparison('Comparison of results for Amplitude Encoding (Degree=1)',\n                clf_amp_c, clf_amp_q, X1, X2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because the problem is not linearly separable, the linear kernel is not\ncapable of solving the problem correctly.\n\nIf the vectors are instead mapped to $d$ copies of the amplitude\nvectors:\n\n\\begin{align}\\phi:\\boldsymbol{x}\\rightarrow\\left|\\psi_\\boldsymbol{x}\\right>\n   ^{\\bigotimes d}\\end{align}\n\nThen the kernel defined is:\n\n\\begin{align}k(\\boldsymbol{x}, \\boldsymbol{x'}) = \\left<\\psi_{\\boldsymbol{x}}|\n   \\psi_{\\boldsymbol{x'}}\\right> \\bigotimes ... \\bigotimes\n   \\left<\\psi_{\\boldsymbol{x}}|\\psi_{\\boldsymbol{x'}}\\right> =\n   \\left(\\frac{1}{|\\boldsymbol{x}||\\boldsymbol{x'}|}\\boldsymbol{x}^T\n   \\boldsymbol{x'}\\right)^d\\end{align}\n\nBy applying a correction of $(|\\boldsymbol{x}||\\boldsymbol{x'}|)^d$\nthis can be used to estimate a polynomial kernel.\n\nWith a higher degree, the problem at hand becomes linearly separable, as the\ninput vectors are being expanded into a higher dimension. This is one of the\nmain features of SVMs and kernels, commonly refered to as the kernel trick.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf_amp_c_2 = SVC(kernel=AmplitudeEncoding(degree=2).classic_kernel)\nclf_amp_c_2.fit(x, y)\nclf_amp_q_2 = SVC(kernel=AmplitudeEncoding(degree=2).quantum_kernel)\nclf_amp_q_2.fit(x, y)\n\nplot_comparison('Comparison of results for Amplitude Encoding (Degree=2)',\n                clf_amp_c_2, clf_amp_q_2, X1, X2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Expanded Amplitude Encoding\n\nExpanded Amplitude Encoding's feature map was identical to regular Amplitude\nEncoding's. The only difference being that the input vectors were expanded\nwith an extra component with value *c*.\n\n\\begin{align}\\phi:\\boldsymbol{x}\\rightarrow\\left|\\psi_\\boldsymbol{x}\\right>=\n       \\frac{1}{\\sqrt{|\\boldsymbol{x}|^2+c^2}}\\left(c\\left|0\\right> +\n       \\sum_{i=1}^{N}x_i\\left|i\\right>\\right)\\end{align}\n\nThis defines a more general polynomial kernel when mapping to $d$\ncopies of the amplitude vector and applying a correction of\n$\\sqrt{|\\boldsymbol{x}|^2+c^2}\\sqrt{|\\boldsymbol{x'}|^2+c^2}$:\n\n\\begin{align}k(\\boldsymbol{x}, \\boldsymbol{x'}) = \\left<\\psi_{\\boldsymbol{x}}|\n   \\psi_{\\boldsymbol{x'}}\\right> = \\left(\\frac{1}{\\sqrt{|\\boldsymbol{x}|^2+\n   c^2}\\sqrt{|\\boldsymbol{x'}|^2+c^2}}(\\boldsymbol{x}^T\\boldsymbol{x'}+c^2)\n   \\right)^d\\end{align}\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "expamp = ExpandedAmplitudeEncoding(degree=2, c=1)\nclf_expamp_c = SVC(kernel=expamp.classic_kernel)\nclf_expamp_c.fit(x, y)\nclf_expamp_q = SVC(kernel=expamp.quantum_kernel)\nclf_expamp_q.fit(x, y)\n\nplot_comparison('Comparison of results for Expanded Amplitude Encoding '\n                '(Degree=2, c=1)',\n                clf_expamp_c, clf_expamp_q, X1, X2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It should be noted that increasing the value of $c$ can have noticeable\nimpacts on the precision of the results. The bigger the value of $c$,\nthe smaller the values of the rest of the components in the vector once\nnormalized. This in turn increases the imprecision of the quantum estimation\nsubroutine.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "expamp_50 = ExpandedAmplitudeEncoding(degree=2, c=50)\nclf_expamp_c_c50 = SVC(kernel=expamp_50.classic_kernel)\nclf_expamp_c_c50.fit(x, y)\nclf_expamp_q_c50 = SVC(kernel=expamp_50.quantum_kernel)\nclf_expamp_q_c50.fit(x, y)\n\nplot_comparison('Comparison of results for Expanded Amplitude Encoding '\n                '(Degree=2, c=50)',\n                clf_expamp_c_c50, clf_expamp_q_c50, X1, X2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Angle Encoding\n\nAngle Encoding's feature map was:\n\n\\begin{align}\\phi:\\boldsymbol{x}\\rightarrow\\left|\\psi_\\boldsymbol{x}\\right>=\n   \\bigotimes_{i=1}^{N}\\cos{x_i}\\left|0\\right>+\\sin{x_i}\\left|1\\right>\\end{align}\n\nThe kernel defined by the inner product is a cosine kernel:\n\n\\begin{align}k(\\boldsymbol{x}, \\boldsymbol{x'}) = \\left<\\psi_{\\boldsymbol{x}}|\n   \\psi_{\\boldsymbol{x'}}\\right> = \\prod_{i=1}^{N}\\sin{x_i}\\sin{x'_i} +\n   \\cos{x_i}\\cos{x'_i}=\\prod_{i=1}^{N}\\cos{(x_i-x'_i)}\\end{align}\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf_ang_c = SVC(kernel=AngleEncoding().classic_kernel).fit(x, y)\nclf_ang_q = SVC(kernel=AngleEncoding().quantum_kernel).fit(x, y)\n\nplot_comparison('Comparison of results for Angle Encoding',\n                clf_ang_c, clf_ang_q, X1, X2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quantum-inspired classical kernels\n\nThe package also contains the submodule `skqlearn.ml.kernels`, responsible\nfor the implementation of classical kernels which have been inspired by\nquantum properties.\n\n## Squeezing Kernel\n\nThis kernel is based on the definition of a *squeezed vacuum state* of the\nelectromagnetic field, defined as:\n\n\\begin{align}\\left|z\\right> = \\frac{1}{\\sqrt{\\cosh(r)}}\n       \\sum_{n=0}^{\\infty}\\frac{\\sqrt{(2n)!}}{2^n n!}\n       (-e^{i\\varphi}\\tanh(r))^n\\left|2n\\right>\\end{align}\n\nInterpreting it as a feature map for real vectors, it defines the following\nkernel:\n\n\\begin{align}k(\\boldsymbol{x}, \\boldsymbol{x}')=\\prod_{i=1}^{N} \\left<(c,x_i)|\n       (c, x_i')\\right>=\\prod_{i=1}^N\\sqrt{\\frac{\\text{sech }c\\text{ sech }c}\n       {1-e^{i(x_i'-x_i)}\\tanh c \\tanh c}}\\end{align}\n\nThis kernel can be evaluated classically by taking the absolute square of the\ninner product.\n\nFor more information on the mathematical reasoning, refer to\n:class:`skqlearn.ml.kernels.SqueezingKernel`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf_squ = SVC(kernel=SqueezingKernel(c=1.0).kernel).fit(x, y)\n\nplot_individual('Results for Squeezing Kernel with (c=1.0)', clf_squ, X1, X2)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}