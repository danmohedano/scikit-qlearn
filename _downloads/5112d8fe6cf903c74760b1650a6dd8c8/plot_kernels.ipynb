{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Kernels\n\nThis tutorial aims to explain and visualize the kernels obtained from the data\nencoding methods implemented in the package.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n\nAs described in the `encodingstutorial` tutorial, the encoding methods\ncan be interpreted as feature maps of the form:\n\n\\begin{align}\\phi : \\mathcal{X} \\rightarrow \\mathcal{F}\\end{align}\n\nWhere the input $x$ is mapped from the input space into the feature\nspace.\n\nThe inner product of two inputs mapped to feature space defines a kernel via:\n\n\\begin{align}k(x,x'):= \\left<\\phi(x), \\phi(x')\\right>_\\mathcal{F}\\end{align}\n\nWhere $\\left<.,.\\right>_\\mathcal{F}$ is the inner product defined\non $\\mathcal{F}$. :cite:`hilbert2019`\n\nIn this case, as the inputs are being mapped into quantum states,\nthe kernel defined is of the form:\n\n\\begin{align}k(x,x')=\\left<\\phi(x)|\\phi(x')\\right>.\\end{align}\n\nBecause of this, the encoding methods can be used to define kernels with the\ninner product and use this with machine learning algorithms such as\nSupport-Vector Machines (SVMs).\n\nTwo methods have been defined in this package, `classic_kernel` and\n`quantum_kernel`, in order to use the kernels defined by the encoding methods\nwith implementations of SVMs such as\n`sklearn's <https://scikit-learn.org/stable/modules/svm.html#svm>`_.\nThese methods compute the Gram matrix of the input set of vectors.\n\nThroughout this tutorial, both the classical computation and quantum\nestimation of the kernels will be tested, so the quantum backend will\nbe initialized with one of the simulators provided by Qiskit.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skqlearn.utils import JobHandler\nfrom skqlearn.encoding import *\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom qiskit.providers.aer import AerSimulator\nfrom sklearn.svm import SVC\n\nJobHandler().configure(backend=AerSimulator(), shots=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basis Encoding\n\nBasis Encoding's feature map was:\n\n\\begin{align}\\phi : i\\rightarrow \\left|i\\right>\\end{align}\n\nTherefore, the kernel defined by the inner product is:\n\n\\begin{align}k(i, j) = \\left<\\phi(i)|\\phi(j)\\right> = \\left<i|j\\right> =\n    \\delta_{ij}\\end{align}\n\nWith $\\delta$ being the Kronecker delta.\n\nTherefore, when computing the Gram matrix of a set of vectors, the expected\nresult would be the identity matrix.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = np.array([[1], [2], [3], [4]])\nprint('Gram matrix with classical computation:')\nprint(BasisEncoding().classic_kernel(x, x))\nprint('Gram matrix with quantum computation:')\nprint(BasisEncoding().quantum_kernel(x, x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Amplitude Encoding\n\nAmplitude Encoding's feature map was:\n\n\\begin{align}\\phi:\\boldsymbol{x}\\rightarrow\\left|\\psi_\\boldsymbol{x}\\right>=\n   \\sum_{i=1}^{N}x_i\\left|i\\right>\\end{align}\n\nTherefore, the kernel defined by the inner product is the linear kernel:\n\n\\begin{align}k(\\boldsymbol{x}, \\boldsymbol{x'}) = \\left<\\psi_{\\boldsymbol{x}}|\n   \\psi_{\\boldsymbol{x'}}\\right> = \\boldsymbol{x}^T\\boldsymbol{x'}.\\end{align}\n\nBecause Amplitude Encoding is limited to normalized data, it can only\nbe used with points in the unit circle (when working in 2D).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "angles = np.arange(0, 2 * np.pi - 0.1, step=np.pi / 6)\nx = np.array([[np.cos(a), np.sin(a)] for a in angles])\ny = np.array([*[0]*6, *[1]*6])\nX0, X1 = x[:, 0], x[:, 1]\nplt.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=60, edgecolors='k')\nplt.show()\n\nclf_amp_c = SVC(kernel=AmplitudeEncoding(degree=1).classic_kernel).fit(x, y)\nclf_amp_q = SVC(kernel=AmplitudeEncoding(degree=1).quantum_kernel).fit(x, y)\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.suptitle('Amplitude Encoding')\nax1.set_title('Classic')\nax2.set_title('Quantum')\nax1.scatter(X0, X1, c=clf_amp_c.predict(x), cmap=plt.cm.coolwarm, s=60,\n            edgecolors='k')\nax2.scatter(X0, X1, c=clf_amp_q.predict(x), cmap=plt.cm.coolwarm, s=60,\n            edgecolors='k')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because the next encodings can be applied to generic points in 2D space,\na couple of utility functions will be defined to help with the visualization\nof the results. Specifically, they will be used to visualize the decision\nboundaries of the SVM.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_meshgrid(x, y, h=.02, border=.25):\n    x_min, x_max = x.min() - border, x.max() + border\n    y_min, y_max = y.min() - border, y.max() + border\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    return xx, yy\n\n\ndef plot_contours(ax, clf, xx, yy, **params):\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    out = ax.contourf(xx, yy, Z, **params)\n    return out\n\n\ndef plot_comparison(title, clf_c, clf_q, X0, X1):\n    xx, yy = make_meshgrid(X0, X1, 0.1)\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.suptitle(title)\n    ax1.set_title('Classic')\n    ax2.set_title('Quantum')\n    plot_contours(ax1, clf_c, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n    plot_contours(ax2, clf_q, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n    ax1.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=60, edgecolors='k')\n    ax2.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=60, edgecolors='k')\n    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to use a simple example which is not linearly separable, the\nproposed data for the SVM to classify is the XOR problem.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny = np.array([0, 1, 1, 0])\nX0, X1 = x[:, 0], x[:, 1]\n\nplt.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=60, edgecolors='k')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Expanded Amplitude Encoding\n\nExpanded Amplitude Encoding's feature map was identical to regular Amplitude\nEncoding's. The only difference being that the input vectors were expanded\nwith an extra component with value *1*.\n\nThis means that with degree equal to *1*, the kernel defined was the linear\nkernel.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf_expamp_c = SVC(kernel=ExpandedAmplitudeEncoding(degree=1).classic_kernel).\\\n    fit(x, y)\nclf_expamp_q = SVC(kernel=ExpandedAmplitudeEncoding(degree=1).quantum_kernel).\\\n    fit(x, y)\n\nplot_comparison('Expanded Amplitude Encoding (Degree=1)', clf_expamp_c,\n                clf_expamp_q, X0, X1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As it can be seen, the kernel is not strictly the linear kernel in 2D. If it\nwas, the decision boundary would be a straight line. The reason behind this\nis that the kernel is linear in 3D (the real dimension of the expanded\nvectors), and therefore the decision boundary is a plane. What is being shown\nin the 2D plot is just the proyection of the plane into a 2D line. This can\nbe seen more clearly when observing how the data is mapped in 3D space.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits import mplot3d\n\nx_3d = np.array([ExpandedAmplitudeEncoding().encoding(x[i, :])\n                 for i in range(x.shape[0])])\nX0_3d, X1_3d, X2_3d = x_3d[:, 0], x_3d[:, 1], x_3d[:, 2]\n\nfig = plt.figure()\nax = plt.axes(projection='3d')\nax.scatter3D(X0_3d, X1_3d, X2_3d, c=y, cmap=plt.cm.coolwarm, s=60,\n              edgecolors='k')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When changing the degree of the encoding (copies of the quantum state),\nthe kernel defined changes to a polynomial kernel of the form:\n\n\\begin{align}\\phi : \\boldsymbol{x}\\rightarrow\\left|\\psi_\\boldsymbol{x}\\right>\n   ^{\\bigotimes d}\\end{align}\n\n\\begin{align}k(\\boldsymbol{x}, \\boldsymbol{x'}) = \\left<\\psi_{\\boldsymbol{x}}|\n   \\psi_{\\boldsymbol{x'}}\\right> \\bigotimes ... \\bigotimes\n   \\left<\\psi_{\\boldsymbol{x}}|\\psi_{\\boldsymbol{x'}}\\right> =\n   (\\boldsymbol{x}^T\\boldsymbol{x'})^d.\\end{align}\n\nWith a higher degree, the problem at hand becomes linearly separable, as the\ninput vectors are being expanded into a higher dimension. This is one of the\nmain features of SVMs and kernels, commonly refered to as the kernel trick.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf_expamp_c = SVC(kernel=ExpandedAmplitudeEncoding(degree=4).classic_kernel).\\\n    fit(x, y)\nclf_expamp_q = SVC(kernel=ExpandedAmplitudeEncoding(degree=4).quantum_kernel).\\\n    fit(x, y)\n\nplot_comparison('Expanded Amplitude Encoding (Degree=4)', clf_expamp_c,\n                clf_expamp_q, X0, X1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Angle Encoding\n\nAngle Encoding's feature map was:\n\n\\begin{align}\\phi:\\boldsymbol{x}\\rightarrow\\left|\\psi_\\boldsymbol{x}\\right>=\n   \\bigotimes_{i=1}^{N}\\cos{x_i}\\left|0\\right>+\\sin{x_i}\\left|1\\right>\\end{align}\n\nThe kernel defined by the inner product is a cosine kernel:\n\n\\begin{align}k(\\boldsymbol{x}, \\boldsymbol{x'}) = \\left<\\psi_{\\boldsymbol{x}}|\n   \\psi_{\\boldsymbol{x'}}\\right> = \\prod_{i=1}^{N}\\sin{x_i}\\sin{x'_i} +\n   \\cos{x_i}\\cos{x'_i}=\\prod_{i=1}^{N}\\cos{(x_i-x'_i)}.\\end{align}\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf_ang_c = SVC(kernel=AngleEncoding().classic_kernel).fit(x, y)\nclf_ang_q = SVC(kernel=AngleEncoding().quantum_kernel).fit(x, y)\n\nplot_comparison('Angle Encoding', clf_ang_c, clf_ang_q, X0, X1)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}